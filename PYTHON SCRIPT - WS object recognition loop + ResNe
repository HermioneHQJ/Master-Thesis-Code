#PYTHON SCRIPT - WS object recognition loop + ResNet
import torch
import torchvision
import requests
import gc
from transformers import (
    YolosFeatureExtractor,
    YolosForObjectDetection,
    AutoImageProcessor,
    BlipProcessor,
    BlipForConditionalGeneration,
    pipeline,
    ResNetForImageClassification,
    AutoTokenizer,
)
from torchvision.models.detection import FasterRCNN
from torchvision.transforms import functional as F
from transformers import YolosFeatureExtractor, YolosForObjectDetection, AutoImageProcessor
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import os
import tqdm
import pandas as pd
from transformers import pipeline
from pillow_heif import register_heif_opener

register_heif_opener()

# Define the COCO labels
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Load the pre-trained Faster R-CNN model
model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model_rcnn.eval()

# Load the YOLOS model for object detection
feature_extractor_yolos = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')
model_yolos = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')
image_processor_yolos = AutoImageProcessor.from_pretrained("hustvl/yolos-small")

# Load the BLIP model for image captioning
processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model_blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Load the image-to-text model
image_to_text = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")

# Load the ResNet-50 model for image classification
classification_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
classification_model = ResNetForImageClassification.from_pretrained("microsoft/resnet-50")

# Modify the process_image function to handle .heic format images
def process_image(image_path):
    try:
        # Convert HEIF image to JPEG format
        if image_path.lower().endswith('.heic'):
            converted_image_path = image_path[:-5] + '.jpg'
            Image.open(image_path).convert('RGB').save(converted_image_path, 'JPEG')
            image_path = converted_image_path

        # Object detection with Faster R-CNN
        image = Image.open(image_path)
        image_tensor = F.to_tensor(image)
        predictions = model_rcnn([image_tensor])

        boxes = predictions[0]['boxes'].tolist()
        labels = predictions[0]['labels'].tolist()
        scores = predictions[0]['scores'].tolist()

        filtered_labels = [COCO_INSTANCE_CATEGORY_NAMES[label] for label in labels]

        # Object detection with YOLOS
        inputs_yolos = feature_extractor_yolos(images=image, return_tensors="pt")
        outputs_yolos = model_yolos(**inputs_yolos)
        target_sizes = torch.tensor([image.size[::-1]])
        results_yolos = image_processor_yolos.post_process_object_detection(outputs_yolos, threshold=0.05, target_sizes=target_sizes)[0]

        yolos_labels = [model_yolos.config.id2label[label.item()] for label in results_yolos["labels"]]
        yolos_boxes = [box.tolist() for box in results_yolos["boxes"]]
        yolos_scores = [score.item() for score in results_yolos["scores"]]

        # Image captioning with BLIP
        inputs_blip = processor_blip(image, return_tensors="pt")
        out = model_blip.generate(**inputs_blip)
        caption_blip = processor_blip.decode(out[0], skip_special_tokens=True)

        # Image-to-text with ViT-GPT2
        text = image_to_text(image_path)[0]['generated_text']

        
         # Image classification with ResNet-50
        inputs_classification = classification_processor(image, return_tensors="pt")
        outputs_classification = classification_model(**inputs_classification)
        predicted_label_idx = outputs_classification.logits.argmax(dim=1).item()
        predicted_scores = outputs_classification.logits.softmax(dim=1)[0].tolist()

        logits = outputs_classification.logits
        
        predicted_label = logits.argmax(-1).item()

        #print("---- Classification with ResNet50_Label completed ---")
        #print(f"{classification_model.config.id2label[predicted_label]}: {predicted_scores[predicted_label]}")
        #print("---- printing all labels ---")

        #print(f"{classification_model.config.id2label}")
        #print("---- printing all scores ---")

        #print(f"{predicted_scores}")

        # Cleanup memory
        del image, image_tensor, predictions, inputs_yolos, outputs_yolos, target_sizes, results_yolos
        torch.cuda.empty_cache()
        gc.collect()

        return {
            "image_path": image_path,
            "FasterRCNN_Labels": filtered_labels,
            "FasterRCNN_Boxes": boxes,
            "FasterRCNN_Scores": scores,
            "YOLOS_Labels": yolos_labels,
            "YOLOS_Boxes": yolos_boxes,
            "YOLOS_Scores": yolos_scores,
            "Caption_blip": caption_blip,
            "Text_gpt2": text,
            "ResNet50_Label": classification_model.config.id2label[predicted_label],
            "ResNet50_Scores": predicted_scores[predicted_label]
        }

    except Exception as e:
        print(f"Error processing image: {image_path}")
        print(e)
        return None

# Set the main directory containing the sub-folders
main_directory = "C:/Users/luigi/OneDrive - University of Glasgow/Photo walks repository"

# Create a list to store the results in the long format
long_format_results = []

# Get the total number of files for the progress bar
total_files = sum([len(files) for _, _, files in os.walk(main_directory)])

# Initialize the progress bar
pbar = tqdm.tqdm(total=total_files, unit="image")


# import os



# Loop through the sub-folders and process the images
for root, dirs, files in os.walk(main_directory):
    for file in files:
        image_path = os.path.join(root, file)
        result = process_image(image_path)
        if result:
            image_path = result["image_path"]

            # Append the Faster R-CNN results to the long format
            for label, box, score in zip(result["FasterRCNN_Labels"], result["FasterRCNN_Boxes"], result["FasterRCNN_Scores"]):
                long_format_results.append({
                    "image_path": image_path,
                    "label": label,
                    "box": box,
                    "score": score,
                    "method": "Faster R-CNN"
                })

            # Append the YOLOS results to the long format
            for label, box, score in zip(result["YOLOS_Labels"], result["YOLOS_Boxes"], result["YOLOS_Scores"]):
                long_format_results.append({
                    "image_path": image_path,
                    "label": label,
                    "box": box,
                    "score": score,
                    "method": "YOLOS"
                })

            # Append the BLIP caption to the long format
            long_format_results.append({
                "image_path": image_path,
                "caption": result["Caption_blip"],
                "method": "BLIP"
            })

            # Append the ViT-GPT2 text to the long format
            long_format_results.append({
                "image_path": image_path,
                "text": result["Text_gpt2"],
                "method": "ViT-GPT2"
            })

            # Append the Resnet50 text to the long format
            long_format_results.append({
                "image_path": image_path,
                "label":result["ResNet50_Label"],
                "score":result["ResNet50_Scores"],
                "method":"ResNet50"
            })

       

        # Update the progress bar
        pbar.update(1)

# Close the progress bar
pbar.close()

# Create a DataFrame from the long format results
long_format_df = pd.DataFrame(long_format_results)

# Save the results to a CSV file
long_format_df.to_csv('C:/Users/luigi/OneDrive - University of Glasgow/Desktop/Preliminary Script/WS_image_analysis_results_long_format.csv', index=False)

# Print the resulting DataFrame
print(long_format_df)

# In this updated pipeline, I added the Image Classification step using the ResNet-50 model. The predicted label #romesNet-50 is included in the final results.
